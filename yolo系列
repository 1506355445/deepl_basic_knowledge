1. yolov1 
    思想：1）将一幅图像分成SxS个网格(grid cell)， 如果某个object的中心 落在这个网格中，则这个网格就负责预测这个object。
         2) 每个网格要预测B个bounding box，每个bounding box 除了要预测位置之外，还要附带预测一个confidence值。 每个网格还要预测C个类别的分数。
    grid cell:
        YOLO将目标检测问题作为回归问题。会将输入图像分成S × S S \times SS×S的网格（cell），如果一个物体的中心点落入到一个cell中，那么该cell就要负责预测该物体，一个格子
 只能预测一个物体，会生成两个预测框。
    损失函数:
        YOLO V1每个网格单元能够预测多个边界框。为了计算true positive的损失，只希望其中一个框负责该目标，为此选择与GT具有最高IOU的那个框。
    YOLO正样本选择
        当一个真实物体的中心点落在了某个cell内时，该cell就负责检测该物体。
        具体做法是将与该真实物体有最大IoU的边框设为正样本， 这个区域的类别真值为该真实物体的类别，该边框的置信度真值为1。
    YOLO负样本选择
        除了上述被赋予正样本的边框，其余边框都为负样本。负样本没有类别损失与边框位置损失，只有置信度损失，其真值为0。
    YOLO使用预测值和GT之间的误差平方的求和（MSE）来计算损失。 损失函数包括
        localization loss -> 坐标损失（预测边界框与GT之间的误差）
        classification loss -> 分类损失
        confidence loss -> 置信度损失（框里有无目标, objectness of the box)
   缺点：
        每个网格只对应两个bounding box，当物体的长宽比不常见(也就是训练数据集覆盖不到时)，效果较差。
        原始图片只划分为7x7的网格，当两个物体靠的很近时，效果比较差。
        最终每个网格只对应一个类别，容易出现漏检(物体没有被识别到)。
        对于图片中比较小的物体，效果比较差。这其实是所有目标检测算法的通病。
        
2.yolov2
    YOLO v1虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLO v2在YOLO v1的基础上提出了几种改进策略
    （1）Batch Normalization（批归一化）
    （2）High Resolution Classifier（高分辨率预训练分类网络）：目前的大部分检测模型都会使用主流分类网络（如vgg、resnet）在ImageNet上的预训练模型作为特征提取器,而这些
 分类网络大部分都是以小于256 × 256的图片作为输入进行训练的，低分辨率会影响模型检测能力。 YOLO v2将输入图片的分辨率提升448 × 448，为了使网络适应新的分辨率，YOLO v2先在
 ImageNet上以448 × 448 的分辨率对网络进行10个epoch的微调，让 网络适应高分辨率的输入。通过使用高分辨率的输入，YOLO v2的mAP提升了约4%。
    （3）Convolutional With Anchor Boxes（带Anchor Box的卷积）
    （4）Dimension Clusters（Anchor Box的宽高由聚类产生）
    （5）New Network：Darknet-19
    （6）Direct location prediction（绝对位置预测）
    （7）Fine-Grained Features（细粒度特征）：YOLO v2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度
 检测。 YOLO v2提取Darknet-19最后一个max pool层的输入，得到26 × 26 × 512的特征图。经过1 × 1 × 64的卷积以降低特征图的维度，得到26 × 26 × 64的特征图，然后经过pass through
 层的处理变成13 × 13 × 256特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍（高宽减半），channel增加4倍），再与13 × 13 × 1024大小的特征 图连接，
 变成13 × 13 × 1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features， YOLO v2的性能提升了1%.
    （8）Multi-Scale Training（多尺寸训练）：YOLO v2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。 YOLO v2采用多尺度输入的方式训练，在
 训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32 3232的倍数{320,352,…,608}。采用Multi-Scale Training, 可以
 适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。


3.yolov3
    改进：调整了网络结构；利用多尺度特征进行对象检测；对象分类用Logistic取代了softmax
    darknet-53借用了resnet的思想，在网络中加入了残差模块，这样有利于解决深层次网络的梯度问题，每个残差模块由两个卷积层和一个shortcut connections,(1,2,8,8,4)代表有几个
重复的残差模块，整个v3结构里面，没有池化层和全连接层，网络的下采样是通过设置卷积的stride为2来达到的，每当通过这个卷积层之后图像的尺寸就会减小到一半。而每个卷积层的实现
又是包含 卷积+BN+Leaky relu，每个残差模块之后又要加上一个zero padding。



