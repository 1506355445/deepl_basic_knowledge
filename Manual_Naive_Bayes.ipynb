{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e15aea-a4d4-4adc-8c9f-0cd0dd7b5e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores of our model are : [98.0, 100.0, 100.0]\n",
      "The average score of our model is : 99.333%\n"
     ]
    }
   ],
   "source": [
    "#1.导入基础库\n",
    "from csv import reader\n",
    "from math import exp,pi,sqrt\n",
    "from random import randrange,seed\n",
    "import copy\n",
    " \n",
    "#2.读取csv文件和转换数据类型\n",
    "#读取csv文件\n",
    "def csv_loader(file):\n",
    "    dataset=list()\n",
    "    with open(file,'r') as f:\n",
    "        csv_reader=reader(f)    \n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "#转换属性为浮点数\n",
    "def str_to_float_converter(dataset):\n",
    "    dataset=dataset[1:]\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            row[i]= float(row[i].strip())\n",
    " \n",
    "#转换标签为整数\n",
    "def str_to_int_converter(dataset):\n",
    "    class_values= [row[-1] for row in dataset]\n",
    "    unique_values= set(class_values)\n",
    "    converter_dict=dict()\n",
    "    for i,value in enumerate(unique_values):\n",
    "        converter_dict[value] = i\n",
    "    for row in dataset:\n",
    "        row[-1] = converter_dict[row[-1] ]\n",
    " \n",
    "#3.K折交叉验证拆分数据\n",
    "def k_fold_cross_validation(dataset,n_folds):\n",
    "    dataset_splitted=list()\n",
    "    fold_size= int(len(dataset)/n_folds)\n",
    "    dataset_copy = list(dataset)\n",
    "    for i in range(n_folds):\n",
    "        fold_data = list()\n",
    "        while len(fold_data) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold_data.append(dataset_copy.pop(index))\n",
    "        dataset_splitted.append(fold_data)\n",
    "    return dataset_splitted\n",
    " \n",
    "#4.计算准确性\n",
    "def calculate_accuracy(actual,predicted):\n",
    "    correct_num = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct_num +=1\n",
    "    accuracy = correct_num/float(len(actual)) *100.0\n",
    "    return accuracy\n",
    " \n",
    "#5.模型测试\n",
    "def mode_test(dataset,algo,n_folds,*args):\n",
    "    dataset_splitted = k_fold_cross_validation(dataset,n_folds)\n",
    "    scores  = list() \n",
    "    for fold in dataset_splitted:\n",
    "        train = copy.deepcopy(dataset_splitted)\n",
    "        train.remove(fold)\n",
    "        train = sum(train, [])\n",
    "        test =list()\n",
    "        test = copy.deepcopy(fold)\n",
    "        predicted = algo(train, test, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy= calculate_accuracy(actual,predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    " \n",
    "#6.数据按字典分类和描述\n",
    "#数据组合成按每一种类进行分类\n",
    "def split_class(dataset):\n",
    "    splitted = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if class_value not in splitted:\n",
    "            splitted[class_value]=list()\n",
    "        splitted[class_value].append(vector)\n",
    "    return splitted\n",
    " \n",
    "#计算每一列（x_i)的均值\n",
    "def calculate_mean(column):\n",
    "    mean = sum(column)/len(column)\n",
    "    return mean\n",
    " \n",
    "#计算每一列（x_i)的标准差\n",
    "def calculate_std(column):\n",
    "    mean = calculate_mean(column)\n",
    "    var = sum([(x - mean )**2  for x in column])/float((len(column)-1))\n",
    "    std = sqrt(var)\n",
    "    return std\n",
    " \n",
    "#描述数据[(mean，std，len)]\n",
    "def describe_data(dataset):\n",
    "    description = [(calculate_mean(column), calculate_std(column),\n",
    "                   len(column)) for column in zip(*dataset)]\n",
    "    del description[-1]\n",
    "    return description\n",
    " \n",
    "#数据按字典分类描述{class:[(mean，std，len)])}\n",
    "def describe_class(dataset):\n",
    "    splitted = split_class(dataset)\n",
    "    descriptions = dict()\n",
    "    for class_value, rows in splitted.items():\n",
    "        descriptions[class_value] = describe_data(rows)\n",
    "    return descriptions\n",
    " \n",
    "#7.计算概率的基础模型\n",
    "def calculate_probability(x,mean,std):\n",
    "    exponent = exp(-((x - mean)**2)/(2 *(std**2)))\n",
    "    probability = (1/(sqrt(2* pi) * std)) *exponent\n",
    "    return probability\n",
    " \n",
    "#8.计算每一行数据的概率\n",
    "def calculate_class_probabilities(dataset,row):\n",
    "    descriptions= describe_class(dataset)\n",
    "    total = sum([descriptions[label][0][-1] for label in descriptions])\n",
    "    pribabilities = dict()\n",
    "    for class_key, class_value  in descriptions.items():\n",
    "        pribabilities [class_key] = class_value[0][-1]/float(total)\n",
    "        for i in range(len(class_value)):\n",
    "            mean,std,count = class_value[i]\n",
    "            pribabilities [class_key] *= calculate_probability(row[i],mean,std)\n",
    "    return pribabilities\n",
    "            \n",
    "#9.每一行数据中找出最好的标签\n",
    "def predict(dataset,row):\n",
    "    pribabilities=calculate_class_probabilities(dataset,row)\n",
    "    best_label,best_probability =None, -1\n",
    "    for class_value, probability in pribabilities.items():\n",
    "        if best_label is None or probability >best_probability:\n",
    "            best_probability = probability\n",
    "            best_label= class_value\n",
    "    return best_label\n",
    " \n",
    "#10.预测测试数据的分类\n",
    "def naive_bayes(train,test):\n",
    "    pedictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(train,row)\n",
    "        pedictions.append(prediction)\n",
    "    return pedictions\n",
    " \n",
    "#11.运行和参数调整\n",
    "seed(5)\n",
    "file='E:\\教学\\数据挖掘\\iris.csv'\n",
    "dataset= csv_loader(file)\n",
    "str_to_float_converter(dataset)\n",
    "dataset=dataset[1:]     \n",
    "str_to_int_converter(dataset)\n",
    "n_folds=3\n",
    "algo=naive_bayes\n",
    "scores = mode_test(dataset,algo,n_folds)\n",
    " \n",
    "print('The scores of our model are : %s' % scores)\n",
    "print('The average score of our model is : %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7de7f-8da7-4d7c-ad61-19238c6ed5a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 为什么需要用到K折交叉验证？K值交叉验证的基本思想是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7074fb-b7b0-42d8-baff-94ec0b7dfc02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 更准确的模型评估\n",
    "传统的训练集-测试集分割（如80-20划分）可能因单次划分的随机性导致评估结果不稳定。K折交叉验证通过多次划分数据集并取平均结果，显著降低了评估的方差。例如，在鸢尾花数据集（150条样本）中，3折交叉验证会将数据分为3份（每份50条），每次用不同部分测试，最终综合3次结果，使得评估更具代表性。\n",
    "\n",
    "#### 充分利用有限数据\n",
    "当数据量较少时（如医学影像或小样本研究），直接划分测试集可能导致训练数据不足。K折交叉验证确保每个样本都被用于训练和测试各一次，最大化数据利用率。例如，3折交叉验证中，每个样本会出现在训练集2次（每次2/3数据）和测试集1次。\n",
    "\n",
    "#### 检测过拟合倾向\n",
    "如果模型在不同折上的表现差异很大（如准确率从90%骤降到60%），可能表明模型对某些数据特征过度敏感，存在过拟合风险。这种波动性通过单一划分难以发现。\n",
    "\n",
    "#### 超参数调优的可靠性\n",
    "在调整模型参数（如朴素贝叶斯的平滑系数）时，K折交叉验证能更稳定地比较不同参数组合的性能，避免因数据划分偏差导致误选次优参数。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
