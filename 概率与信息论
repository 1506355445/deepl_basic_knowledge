1.概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方法，也提供了用于导出新的不确定性声明的公理。在AI领域，概率论主要有两种用途：
    首先，概率法则告诉我们AI系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的表达式；
    其次，可以用概率和统计从理论上分析我们提出的AI系统的行为。
    
2.不确定性的3种可能的来源：
    1）被建模系统内在的随机性
    2）不完全观测
    3）不完全建模
 
3.频率派概率和贝叶斯概率
    频率派概率：概率直接与事件发生的频率相联系，被称为频率派概率。（例如，抽取一手牌）
    贝叶斯概率：用概率来表示一种信任度，涉及确定性水平。（例如，医生诊断病人）
    
 4.随机变量和概率分布
    一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。
    概率分布用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。
    离散型变量的概率分布可以用概率质量函数（PMF）来描述，将随机变量能偶取得的每个状态映射到随机变量取得该状态的概率。
    连续性变量的概率分布用概率密度函数（PDF）来描述，它并没有直接对特定的状态给出概率，相对的，它给出了落在面积为δx的无限小的区域内的概率为p(x)δx。

5.协方差和相关系数
    协方差在某种意义上给出两个变量线性相关性的强度以及这些变量的尺度
    相关系数将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响
    两个变量相互独立，协方差为0；协方差不为0，那么它们一定是相关的；协方差为0，不一定独立，但一定没有线性关系
    协方差矩阵对角元是方差
    
6.范畴分布和多项式分布
   范畴分布：试验次数1次，结果可能数有多个。例子：扔一次骰子，3点向上的概率
   多项式分布：试验次数有多次，结果可能数有多个。例子：扔一次骰子，分别为1，2，3点

7.指数分布和拉普拉斯分布
   在深度学习中，我们经常会需要一个在x=0点处取得边界点的分布。为了实现这一目的，我们可以使用指数分布。
   拉普拉斯分布是一种连续概率分布。由于它可以看作是两个不同位置的指数分布背靠背拼在一起，所以它也叫双指数分布。
   与正态分布对比，正态分布是用相对于μ平均值的差的平方来表示，而拉普拉斯概率密度用相对于差的绝对值来表示。因此，拉普拉斯分布的尾部比正态分布更加平坦。
   
 8.狄拉克δ函数和迪利克雷分布
    狄拉克δ函数 或简称 δ函数，定义是在除 0 外其他点都为0，积分为 1 的函数。原点处无限高无限细，总面积为 1。
    在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通过狄拉克δ函数定义概率密度函数来实现：
    p(x)=δ(x-μ)
    
 9. 信息论
 对一个信号包含信息的多少进行量化，可用于描述概率分布或量化概率分布之间的相似性。
 自信息：
 1）非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
 2）较不可能发生的事件具有更高的信息量。
 3）独立事件应具有增量的信息。
 
 10.KL散度和交叉熵
 如果对于同一个随机变量有两个单独的概率分布P(x)和Q(x)，可以使用KL散度来衡量这两个分布的差异：E( log(P(x)/Q(x)) )=E( logP(x)-logQ(x) )
 KL散度具有非负的性质，KL散度为0，当且仅当P和Q在离散型变量的情况下是相同的分布。
 交叉熵和KL散度很像，但是缺少左边一项：-E(logQ(x))
 针对Q最小化交叉熵等价于最小化KL散度，因为Q并不参与被省略的那一项。
